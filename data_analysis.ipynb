{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666a19ab",
   "metadata": {},
   "source": [
    "# Spam Detection - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16053184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join('..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from src.utils import load_config, get_project_root, print_text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca18918",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "- Load raw training and test data from location in configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "train_path = os.path.join(get_project_root(), config['data']['task1']['raw']['train'])\n",
    "test_path = os.path.join(get_project_root(), config['data']['task1']['raw']['test'])\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a403d",
   "metadata": {},
   "source": [
    "- Display basic information about our test and training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Shape:\", train_df.shape)\n",
    "print(\"Test Data Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2e2b5",
   "metadata": {},
   "source": [
    "- Choose a spam and not spam message from the data, use the given function to see if it is spam or not, then print it. We can use this to get a rough idea of what a spam message might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_spam_sample = train_df[train_df['label'] == 0].iloc[0]\n",
    "print(\"NON-SPAM SAMPLE:\")\n",
    "print_text(non_spam_sample['text'], non_spam_sample[-1])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "spam_sample = train_df[train_df['label'] == 1].iloc[0]\n",
    "print(\"SPAM SAMPLE:\")\n",
    "print_text(spam_sample['text'], spam_sample['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023924f",
   "metadata": {},
   "source": [
    "## 2. Analysing Data\n",
    "- Pie chart of spam vs. non-spam distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ee7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(series, mapping):\n",
    "    return series.map(mapping).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0: \"NotSpam\", 1: \"Spam\"}\n",
    "plot_labels = [\"NotSpam\", \"Spam\"]\n",
    "\n",
    "train_labels = map_labels(train_df.iloc[:, -1], mapping)\n",
    "train_counts = [(train_labels == \"NotSpam\").sum(), (train_labels == \"Spam\").sum()]\n",
    "\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.pie(\n",
    "    train_counts,\n",
    "    labels=plot_labels,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['paleturquoise', 'orchid']\n",
    ")\n",
    "\n",
    "ax.set_title('Training Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b0693",
   "metadata": {},
   "source": [
    "- Print key information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Set - NotSpam: {train_counts[0]}, Spam: {train_counts[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1673d3b",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis\n",
    "- Calculate average text length of a spam message vs a non-spam message.\n",
    "- Plot distribution on histogram of average text lengths for spam vs. non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = train_df.iloc[:, 0].apply(len)\n",
    "spam_lengths = text_lengths[train_labels == \"Spam\"]\n",
    "nonspam_lengths = text_lengths[train_labels == \"NotSpam\"]\n",
    "\n",
    "spam_avg_len = spam_lengths.mean()\n",
    "nonspam_avg_len = nonspam_lengths.mean()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(nonspam_lengths, bins=30, alpha=0.6, label='NotSpam', color='paleturquoise')\n",
    "plt.hist(spam_lengths, bins=30, alpha=0.6, label='Spam', color='orchid')\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average text length for spam: {spam_avg_len:.2f} characters\")\n",
    "print(f\"Average text length for non-spam: {nonspam_avg_len:.2f} characters\")\n",
    "print(f\"Max spam length: {spam_lengths.max()}\")\n",
    "print(f\"Number of spam messages > 500 characters: {(spam_lengths > 500).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baceb439",
   "metadata": {},
   "source": [
    "## 4. Linguistic Features Analysis\n",
    "- Average sentence length comparison\n",
    "- Word count distributions\n",
    "- Special character usage (e.g., exclamation marks)\n",
    "- Uppercase word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train_df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_avg_sentence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    return np.mean([len(word_tokenize(sent)) for sent in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sent_lens = texts.apply(length_avg_sentence)\n",
    "\n",
    "labels = pd.Series(map_labels(train_df.iloc[:, -1], mapping))\n",
    "\n",
    "spam_avg_sent_len = avg_sent_lens[labels == \"Spam\"].mean()\n",
    "nonspam_avg_sent_len = avg_sent_lens[labels == \"NotSpam\"].mean()\n",
    "\n",
    "print(f\"Average Spam Sentence Length: {spam_avg_sent_len:.2f} words\")\n",
    "print(f\"Average Not Spam Sentence Length: {nonspam_avg_sent_len:.2f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09037d06",
   "metadata": {},
   "source": [
    "- Word count distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = texts.apply(lambda x: len(word_tokenize(x)))\n",
    "spam_word_counts = word_counts[labels == \"Spam\"]\n",
    "nonspam_word_counts = word_counts[labels == \"NotSpam\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(nonspam_word_counts, bins=30, alpha=0.6, label='NotSpam', color='paleturquoise')\n",
    "plt.hist(spam_word_counts, bins=30, alpha=0.6, label='Spam', color='orchid')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb349f",
   "metadata": {},
   "source": [
    "- Usage of special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = ['!', '?', '$', '%', '&', '@', '*']\n",
    "\n",
    "# Function to count special characters\n",
    "def count_special_chars(text, chars=special_chars):\n",
    "    return sum(text.count(c) for c in chars)\n",
    "\n",
    "# Apply function\n",
    "special_char_counts = texts.apply(lambda x: count_special_chars(x))\n",
    "\n",
    "# Separate counts by label\n",
    "spam_special_char_avg = special_char_counts[labels == \"Spam\"].mean()\n",
    "nonspam_special_char_avg = special_char_counts[labels == \"NotSpam\"].mean()\n",
    "\n",
    "print(f\"Avg Special Characters per Spam Message: {spam_special_char_avg:.2f}\")\n",
    "print(f\"Avg Special Characters per Not Spam Message: {nonspam_special_char_avg:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc769fb7",
   "metadata": {},
   "source": [
    "- Exclamation mark frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c03f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def count_punctuation(text):\n",
    "    # Count all punctuation marks in a message\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "punctuation_counts = texts.apply(count_punctuation)\n",
    "\n",
    "# Calculate averages\n",
    "spam_punctuation_avg = punctuation_counts[labels == \"Spam\"].mean()\n",
    "nonspam_punctuation_avg = punctuation_counts[labels == \"NotSpam\"].mean()\n",
    "\n",
    "print(f\"Avg Punctuation Marks per Spam Message: {spam_punctuation_avg:.2f}\")\n",
    "print(f\"Avg Punctuation Marks per Not Spam Message: {nonspam_punctuation_avg:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e5e6e",
   "metadata": {},
   "source": [
    "- Exclamation mark density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclamation_density(text):\n",
    "    word_count = len(word_tokenize(text))\n",
    "    excl_count = text.count('!')\n",
    "    return excl_count / word_count if word_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclamation_density_counts = texts.apply(exclamation_density)\n",
    "\n",
    "spam_excl_density_avg = exclamation_density_counts[labels == \"Spam\"].mean()\n",
    "nonspam_excl_density_avg = exclamation_density_counts[labels == \"NotSpam\"].mean()\n",
    "\n",
    "print(f\"Average exclamation mark density per message (Spam): {spam_excl_density_avg:.4f}\")\n",
    "print(f\"Average exclamation mark density per message (NotSpam): {nonspam_excl_density_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f147648",
   "metadata": {},
   "source": [
    "## 5. Word Frequency Analysis\n",
    "- Most common words in spam\n",
    "- Most common words in legitimate emails\n",
    "- Create word clouds for visual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fd09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(texts):\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=20)  # Get top 20 words, ignoring stop words\n",
    "    word_matrix = vectorizer.fit_transform(texts)\n",
    "    word_freq = pd.DataFrame(word_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    word_counts = word_freq.sum(axis=0).sort_values(ascending=False)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(texts, title, max_words=20):\n",
    "    text = \" \".join(texts)\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        stopwords=None,\n",
    "        max_words=max_words\n",
    "    ).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_texts = texts[labels == \"Spam\"]\n",
    "nonspam_texts = texts[labels == \"NotSpam\"]\n",
    "\n",
    "# Get most common words in Spam and Non-Spam\n",
    "spam_word_counts = get_most_common_words(spam_texts)\n",
    "nonspam_word_counts = get_most_common_words(nonspam_texts)\n",
    "\n",
    "print(\"Most Common Words in Spam:\")\n",
    "print(spam_word_counts)\n",
    "print(\"\\nMost Common Words in Non-Spam:\")\n",
    "print(nonspam_word_counts)\n",
    "\n",
    "generate_word_cloud(spam_texts, 'Word Cloud for Spam')\n",
    "generate_word_cloud(nonspam_texts, 'Word Cloud for Non-Spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76808051",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis\n",
    "- Correlation between text length and spam classification\n",
    "- Correlation between sentence length and spam classification\n",
    "- Identify other potential correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.DataFrame({\n",
    "    'text_length': text_lengths,\n",
    "    'avg_sentence_length': avg_sent_lens,\n",
    "    'special_char_count': special_char_counts,\n",
    "    'punctuation_count': punctuation_counts,\n",
    "    'excl_density': exclamation_density_counts,\n",
    "})\n",
    "\n",
    "correlations = features_df.corr()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlations)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(\"Spam Detection Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d9623",
   "metadata": {},
   "source": [
    "## 7. Summary of Findings\n",
    "- Key differences between spam and non-spam emails\n",
    "- Potential features for ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7842b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of Findings\")\n",
    "print(\"\\n\")\n",
    "print(\"Differences between spam and non-spam emails:\")\n",
    "\n",
    "if spam_avg_len > nonspam_avg_len:\n",
    "    print(f\"- Spam emails tend to be longer ({spam_avg_len:.1f} vs {nonspam_avg_len:.1f} characters)\")\n",
    "else:\n",
    "    print(f\"- Non-spam emails tend to be longer ({nonspam_avg_len:.1f} vs {spam_avg_len:.1f} characters)\")\n",
    "\n",
    "if spam_special_char_avg > nonspam_special_char_avg:\n",
    "    print(f\"- Spam emails use significantly more special characters ({spam_special_char_avg:.2f} vs {nonspam_special_char_avg:.2f})\")\n",
    "\n",
    "print(\"Vocabulary Patterns:\")\n",
    "print(f\"- Common in spam: {', '.join(spam_word_counts.head(5).index)}\")\n",
    "print(f\"- Common in non-spam: {', '.join(nonspam_word_counts.head(5).index)}\")\n",
    "\n",
    "print(\"Potential Model Features (based on correlation):\")\n",
    "potential_correlations = correlations.where(~np.eye(correlations.shape[0], dtype=bool))\n",
    "\n",
    "for feature in correlations.columns:\n",
    "    related = potential_correlations[feature].dropna().abs().sort_values(ascending=False)\n",
    "    if not related.empty and related.iloc[0] > 0.2:\n",
    "        top_related_feature = related.index[0]\n",
    "        top_corr_value = correlations.loc[feature, top_related_feature]\n",
    "        print(f\"- {feature} correlates with {top_related_feature} (r = {top_corr_value:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
