{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6035b3f",
   "metadata": {},
   "source": [
    "# Spam Detection - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb0762e",
   "metadata": {},
   "source": [
    "- Add the project's root directory (two levels up) to the Python path so the modules can be imported, even if they arent in the current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join('..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b6460",
   "metadata": {},
   "source": [
    "- Import the required libraries and modules, as well as our utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from src.utils import load_config, print_text, save_as_csv, confirm_checksum, get_project_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328f158",
   "metadata": {},
   "source": [
    "- Load the config using the utility function. Get paths to relevant folders/files needed to save and retrieve files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "raw_test_data_checksum = config['data']['task1']['raw']['test_checksum']\n",
    "raw_train_data_checksum = config['data']['task1']['raw']['train_checksum']\n",
    "\n",
    "raw_test_data_path = config['data']['task1']['raw']['test']\n",
    "raw_train_data_path = config['data']['task1']['raw']['train']\n",
    "\n",
    "processed_test_data_path = config['data']['task1']['processed']['test']\n",
    "processed_train_data_path = config['data']['task1']['processed']['train']\n",
    "\n",
    "raw_test_data = os.path.join(get_project_root(), raw_test_data_path.replace('/', os.sep))\n",
    "raw_train_data = os.path.join(get_project_root(), raw_train_data_path.replace('/', os.sep))\n",
    "processed_test_data = os.path.join(get_project_root(), processed_test_data_path.replace('/', os.sep))\n",
    "processed_train_data = os.path.join(get_project_root(), processed_train_data_path.replace('/', os.sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b5733",
   "metadata": {},
   "source": [
    "- Using the provided checksums, check that the test and training data are correctly loaded - ensures consistency with provided files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d10d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if confirm_checksum(raw_test_data, raw_test_data_checksum) and confirm_checksum(raw_train_data, raw_train_data_checksum):\n",
    "    print(\"Training and Testing Data Loaded Correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6c5da",
   "metadata": {},
   "source": [
    "- Load the CSV data into dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(raw_train_data)\n",
    "test_df = pd.read_csv(raw_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475deaa",
   "metadata": {},
   "source": [
    "- Word tokenise the text in both datasets and give this a new column called tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['text'].apply(word_tokenize)\n",
    "test_df['tokens'] = test_df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a542b",
   "metadata": {},
   "source": [
    "- Inspect the dataframes to get an idea of what is going on inside the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd30ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d36371",
   "metadata": {},
   "source": [
    "- For consistency, we apply lowercasing to the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(lambda tokens: [word.lower() for word in tokens])\n",
    "test_df['tokens'] = test_df['tokens'].apply(lambda tokens: [word.lower() for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3caa5ef",
   "metadata": {},
   "source": [
    "- As per the lab exercises, we replace numerical and ordinal data with Nth or NUM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f06c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_convert(tokens):\n",
    "    return [\n",
    "        \"Nth\" if (token.endswith((\"nd\", \"st\", \"th\")) and token[:-2].isdigit())\n",
    "        else \"NUM\" if token.isdigit()\n",
    "        else token\n",
    "        for token in tokens\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e065a3",
   "metadata": {},
   "source": [
    "- Apply the above conversion function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(num_convert)\n",
    "test_df['tokens'] = test_df['tokens'].apply(num_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db83efb",
   "metadata": {},
   "source": [
    "- As per the lab exercises, we want to remove any stopwords - they will not be useful in this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c520c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cca5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "test_df['tokens'] = test_df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e71106",
   "metadata": {},
   "source": [
    "- As learnt from the data analysis, we want to keep symbols as they can help us detect spam or not. We filter out anything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c90220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    allowed_symbols = {\"!\", \"?\", \"$\", \"%\", \"&\", \"@\", \"*\"}\n",
    "    return [word for word in tokens if word.isalpha() or word in {\"NUM\", \"Nth\"} or word in allowed_symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33582c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(filter_tokens)\n",
    "test_df['tokens'] = test_df['tokens'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174e36e",
   "metadata": {},
   "source": [
    "- Use lemmatising and stemming to put words into their base form - makes it more efficient and consistent when training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_or_ordinal(token):\n",
    "    allowed = {\"NUM\", \"Nth\", \"!\", \"?\", \"$\", \"%\", \"&\", \"@\", \"*\"}\n",
    "    return token in allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff90d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_then_stem(tokens):\n",
    "    lemmatised = [lemmatiser.lemmatize(token) if not numerical_or_ordinal(token) else token for token in tokens]\n",
    "    stemmed = [stemmer.stem(token) if not numerical_or_ordinal(token) else token for token in lemmatised]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9230c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(lemmatise_then_stem)\n",
    "test_df['tokens'] = test_df['tokens'].apply(lemmatise_then_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5052e",
   "metadata": {},
   "source": [
    "- Inspect the dataframes to get an idea of what is going on inside the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cacae",
   "metadata": {},
   "source": [
    "- Reconstruct to make it cleaner and easier to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336047a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "test_df['clean_text'] = test_df['tokens'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcb28f",
   "metadata": {},
   "source": [
    "- Inspect a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_df.sample(3)\n",
    "\n",
    "for _, row in train_samples.iterrows():\n",
    "    print_text(row['clean_text'], row['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87554c7",
   "metadata": {},
   "source": [
    "- Inspect the dataframes to get an idea of what is going on inside the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efddaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f50939",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd115d61",
   "metadata": {},
   "source": [
    "- Drop columns that don't matter. We don't need to use tokens anymore as we have processed the data. Text will not be useful because it is not processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['tokens'], axis=1)\n",
    "test_df = test_df.drop(['tokens'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75dd89",
   "metadata": {},
   "source": [
    "- Save the data to the required location with specified file name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1627af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(train_df, processed_train_data, \"spam_detection_train_processed.csv\")\n",
    "save_as_csv(test_df, processed_test_data, \"spam_detection_test_processed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
