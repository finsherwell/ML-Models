{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6035b3f",
   "metadata": {},
   "source": [
    "# Spam Detection - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join('..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from src.utils import load_config, print_text, save_as_csv, confirm_checksum, get_project_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0b5a2",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "raw_test_data_checksum = config['data']['task1']['raw']['test_checksum']\n",
    "raw_train_data_checksum = config['data']['task1']['raw']['train_checksum']\n",
    "\n",
    "raw_test_data_path = config['data']['task1']['raw']['test']\n",
    "raw_train_data_path = config['data']['task1']['raw']['train']\n",
    "\n",
    "processed_test_data_path = config['data']['task1']['processed']['test']\n",
    "processed_train_data_path = config['data']['task1']['processed']['train']\n",
    "\n",
    "raw_test_data = os.path.join(get_project_root(), raw_test_data_path.replace('/', os.sep))\n",
    "raw_train_data = os.path.join(get_project_root(), raw_train_data_path.replace('/', os.sep))\n",
    "processed_test_data = os.path.join(get_project_root(), processed_test_data_path.replace('/', os.sep))\n",
    "processed_train_data = os.path.join(get_project_root(), processed_train_data_path.replace('/', os.sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d10d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if confirm_checksum(raw_test_data, raw_test_data_checksum) and confirm_checksum(raw_train_data, raw_train_data_checksum):\n",
    "    print(\"Training and Testing Data Loaded Correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(raw_train_data)\n",
    "test_df = pd.read_csv(raw_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a4cd2",
   "metadata": {},
   "source": [
    "## 2. Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['text'].apply(word_tokenize)\n",
    "test_df['tokens'] = test_df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd30ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc23bbf",
   "metadata": {},
   "source": [
    "## 3. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(lambda tokens: [word.lower() for word in tokens])\n",
    "test_df['tokens'] = test_df['tokens'].apply(lambda tokens: [word.lower() for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d865051",
   "metadata": {},
   "source": [
    "## 4. Number Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f06c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_convert(tokens):\n",
    "    return [\n",
    "        \"Nth\" if (token.endswith((\"nd\", \"st\", \"th\")) and token[:-2].isdigit())\n",
    "        else \"NUM\" if token.isdigit()\n",
    "        else token\n",
    "        for token in tokens\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(num_convert)\n",
    "test_df['tokens'] = test_df['tokens'].apply(num_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04dcaa1",
   "metadata": {},
   "source": [
    "## 5. Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c520c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cca5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "test_df['tokens'] = test_df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5d518",
   "metadata": {},
   "source": [
    "## 6. Filter and Clean Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c90220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    allowed_symbols = {\"!\", \"?\", \"$\", \"%\", \"&\", \"@\", \"*\"}\n",
    "    return [word for word in tokens if word.isalpha() or word in {\"NUM\", \"Nth\"} or word in allowed_symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33582c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['tokens'].apply(filter_tokens)\n",
    "test_df['tokens'] = test_df['tokens'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2de1a",
   "metadata": {},
   "source": [
    "## 7. Lemmatisation and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_or_ordinal(token):\n",
    "    return token in {\"NUM\", \"Nth\", \"!\", \"?\", \"$\", \"%\", \"&\", \"@\", \"*\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff90d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_then_stem(tokens):\n",
    "    lemmatised = [lemmatiser.lemmatize(token) if not numerical_or_ordinal(token) else token for token in tokens]\n",
    "    stemmed = [stemmer.stem(token) if not numerical_or_ordinal(token) else token for token in lemmatised]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9230c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "train_df['tokens'] = train_df['tokens'].apply(lemmatise_then_stem)\n",
    "test_df['tokens'] = test_df['tokens'].apply(lemmatise_then_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcea52a",
   "metadata": {},
   "source": [
    "## 8. Reconstruct and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cacae",
   "metadata": {},
   "source": [
    "- Reconstruct to make it cleaner and easier to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336047a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "test_df['clean_text'] = test_df['tokens'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcb28f",
   "metadata": {},
   "source": [
    "- Inspect a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_df.sample(3)\n",
    "\n",
    "for _, row in train_samples.iterrows():\n",
    "    print_text(row['clean_text'], row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efddaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f50939",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['text', 'tokens'], axis=1)\n",
    "test_df = test_df.drop(['text', 'tokens'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f10a5",
   "metadata": {},
   "source": [
    "## 9. Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1627af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(train_df, processed_train_data, \"spam_detection_train_processed.csv\")\n",
    "save_as_csv(test_df, processed_test_data, \"spam_detection_test_processed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
